{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) for Facial Expression Recognition\n",
    "## Alex Lauro\n",
    "## APMA 4990 - Intro to Data Science Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: Neutral \n",
      "\n",
      "Anger: 19.41%\n",
      "Disgust: 4.48%\n",
      "Fear: 18.57%\n",
      "Happiness: 11.16%\n",
      "Sadness: 16.46%\n",
      "Surprise: 8.57%\n",
      "Neutral: 21.36%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvwA+JizyQfBCKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105ec1410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from flask import Flask\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "# Open pickled weights and biases from trained CNN\n",
    "\n",
    "fileObject1 = open(\"val_w1\", 'r')\n",
    "w1_val = pickle.load(fileObject1)\n",
    "\n",
    "fileObject2 = open(\"val_w2\", 'r')\n",
    "w2_val = pickle.load(fileObject2)\n",
    "\n",
    "fileObject3 = open(\"val_b1\", 'r')\n",
    "b1_val = pickle.load(fileObject3)\n",
    "\n",
    "fileObject4 = open(\"val_b2\", 'r')\n",
    "b2_val = pickle.load(fileObject4)\n",
    "\n",
    "fileObject5 = open(\"val_w_fc1\", 'r')\n",
    "w_fc1_val = pickle.load(fileObject5)\n",
    "\n",
    "fileObject6 = open(\"val_w_fc2\", 'r')\n",
    "w_fc2_val = pickle.load(fileObject6)\n",
    "\n",
    "fileObject7 = open(\"val_w_fc3\", 'r')\n",
    "w_fc3_val = pickle.load(fileObject7)\n",
    "\n",
    "fileObject8 = open(\"val_b_fc1\", 'r')\n",
    "b_fc1_val = pickle.load(fileObject8)\n",
    "\n",
    "fileObject9 = open(\"val_b_fc2\", 'r')\n",
    "b_fc2_val = pickle.load(fileObject9)\n",
    "\n",
    "fileObject10 = open(\"val_b_fc3\", 'r')\n",
    "b_fc3_val = pickle.load(fileObject10)\n",
    "\n",
    "fileObject11 = open(\"pixel_mean\", 'r')\n",
    "pixel_mean = pickle.load(fileObject11)\n",
    "\n",
    "fileObject12 = open(\"pixel_std\", 'r')\n",
    "pixel_std = pickle.load(fileObject12)\n",
    "\n",
    "# Helper methods\n",
    "\n",
    "def resize_image(image, base_width=48):\n",
    "    \"\"\"\n",
    "    Resizes input image to specified and adjusts height proportionally.\n",
    "    \n",
    "    Args:\n",
    "    *image* - input image\n",
    "    *base_width* - desired width of resized image\n",
    "    \n",
    "    Output:\n",
    "    *image* - image with width *base_width* and proportional height \n",
    "    \"\"\"\n",
    "    \n",
    "    width_percent = (base_width / float(image.size[0]))\n",
    "    height_size = int((float(image.size[1]) * float(width_percent)))\n",
    "    image = image.resize((base_width, height_size), Image.ANTIALIAS)\n",
    "    return image\n",
    "\n",
    "def image_to_grayscale_pixel_values(image):\n",
    "    \"\"\"\n",
    "    Converts an image into a flattened array of pixel values. The image is \n",
    "    then converted to grayscale format so that ixel values range from 0-255 \n",
    "    (0 = black, 255 = white).\n",
    "    \n",
    "    Args:\n",
    "    *image* - input image\n",
    "    \n",
    "    Output:\n",
    "    *flat_pixels_array* - flattened array of image pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert image to 8-bit grayscale\n",
    "    img = image.convert('L')  \n",
    "    #img.save('/Users/Alex/Desktop/greyscale.png')\n",
    "    \n",
    "    width, height = img.size\n",
    "    # Convert image data to a list of integers\n",
    "    pixels = list(img.getdata()) \n",
    "    # Convert that to 2D list (list of lists of integers)\n",
    "    pixels = [pixels[offset:offset + width] for offset in range(0, width * height, width)]\n",
    "    # Convert list of lists to flattened array\n",
    "    pixels_array = np.asarray(pixels)\n",
    "    flat_pixels_array = pixels_array.flatten()\n",
    "    return flat_pixels_array\n",
    "\n",
    "def image_size(image):\n",
    "    \"\"\"\n",
    "    Return image width and height\n",
    "    \n",
    "    Args:\n",
    "    *image* - input image\n",
    "    \n",
    "    Output:\n",
    "    *image_width* - image width\n",
    "    *image_height* - image height\n",
    "    \"\"\"\n",
    "    \n",
    "    image_width, image_height = image.size\n",
    "    return image_width, image_height\n",
    "\n",
    "def crop_center(image, side_length=48):\n",
    "    \"\"\"\n",
    "    Crops the center of an image into a square image\n",
    "    \n",
    "    Args:\n",
    "    *image* - input image\n",
    "    *side_length* - side length of cropped square image\n",
    "    \n",
    "    Output:\n",
    "    *image_crop* - cropped image \n",
    "    \"\"\"\n",
    "    \n",
    "    half_width = np.floor(image.size[0] / 2)\n",
    "    half_height = np.floor(image.size[1] / 2)\n",
    "    \n",
    "    image_crop = img.crop(\n",
    "        (\n",
    "            half_width - (side_length / 2) - 1,\n",
    "            half_height - (side_length / 2),\n",
    "            half_width + (side_length / 2) - 1,\n",
    "            half_height + (side_length / 2)\n",
    "        )\n",
    "    )\n",
    "    return image_crop\n",
    "\n",
    "def standardize_pixels(image_pixels, pixel_mean, pixel_std):\n",
    "    \"\"\"\n",
    "    Standardizes *image_pixels* by first adjusting image norm to 100 and\n",
    "    then subtracting the mean and dividing by the standard deviation of training \n",
    "    set pixel mean and pixel standard deviation\n",
    "    \n",
    "    Args:\n",
    "    *image_pixels* - flattened array of grayscale image pixels\n",
    "    *pixel_mean* - training set pixel mean\n",
    "    *pixed_std* - training set pixel standard deviation\n",
    "    \n",
    "    Output:\n",
    "    *image_pixels* - standardized flattened array of grayscale image pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    #img_pixels = img_pixels - img_pixels.mean(axis=1).reshape(-1, 1)\n",
    "    image_pixels = np.multiply(image_pixels, 100.0 / 255.0)\n",
    "    image_pixels = np.divide(np.subtract(image_pixels, np.mean(pixel_mean)), np.mean(pixel_std))\n",
    "    return image_pixels\n",
    "\n",
    "def pre_process_image(image, pixel_mean, pixel_std):\n",
    "    \"\"\"\n",
    "    Pre-processes image by calling resize_image, crop_center, image_to_grayscale_pixel_values,\n",
    "    and standardize_pixels functions.\n",
    "    \n",
    "    Args:\n",
    "    *image* - input image\n",
    "    *pixel_mean* - training set pixel mean\n",
    "    *pixed_std* - training set pixel standard deviation\n",
    "    \n",
    "    Output:\n",
    "    *pre_processed_image_pixels* - standardized flattened array of grayscale image pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    image = resize_image(image)\n",
    "    image = crop_center(image)\n",
    "    image_pixels = image_to_grayscale_pixel_values(image)\n",
    "    image_pixels = standardize_pixels(image_pixels, pixel_mean, pixel_std)\n",
    "    return image_pixels\n",
    "\n",
    "emotion_labels = np.array([0, 1, 2, 3, 4, 5, 6])\n",
    "emotion_names = np.array(['Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise', 'Neutral'])\n",
    "num_emotions = emotion_labels.shape[0]\n",
    "\n",
    "# CNN code\n",
    "\n",
    "K = 64 # First convolutional layer output depth \n",
    "L = 128 # Second convolutional layer output depth (128 = 64 * 2)\n",
    "M = 3072 # Fully connected layer 1 depth (3072 = 128 * 24)\n",
    "\n",
    "def CNN(img_pixels):\n",
    "    # Image placeholder\n",
    "    x = tf.placeholder('float', shape=[None, img_pixels.shape[0]])\n",
    "    image = tf.reshape(x, [-1, np.sqrt(img_pixels.shape[0]).astype(np.uint8), np.sqrt(img_pixels.shape[0]).astype(np.uint8), 1])\n",
    "\n",
    "    # First convolutional layer with depth 64 and 5x5 filter size\n",
    "    w1 = tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=1e-4))\n",
    "    b1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(image, w1, strides=[1, 1, 1, 1], padding=\"SAME\") + b1)\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "    # Second convolutional layer with depth 128 and 5x5 filter size\n",
    "    w2 = tf.Variable(tf.truncated_normal([5, 5, 64, L], stddev=1e-4))\n",
    "    b2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "\n",
    "    conv2 = tf.nn.relu(tf.nn.conv2d(norm1, w2, strides=[1, 1, 1, 1], padding=\"SAME\") + b2)\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    w_fc1 = tf.Variable(tf.truncated_normal([12 * 12 * L, M], stddev=0.04))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.0, tf.float32, [M]))\n",
    "\n",
    "    pool2_loc = tf.reshape(pool2, [-1, 12 * 12 * L])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(pool2_loc, w_fc1) + b_fc1)\n",
    "    \n",
    "    # Fully connected layer 2\n",
    "    w_fc2 = tf.Variable(tf.truncated_normal([3072, 1536], stddev=0.04))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.0, tf.float32, [1536]))\n",
    "\n",
    "    h_fc2_reshape = tf.reshape(h_fc1, [-1, 3072])\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc2_reshape, w_fc2) + b_fc2)\n",
    "\n",
    "    # Dropout\n",
    "    keep_prob = tf.placeholder('float')\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    # Final fully connected layer feeding into softmax classifier\n",
    "    w_fc3 = tf.Variable(tf.truncated_normal([1536, num_emotions], stddev=0.04))\n",
    "    b_fc3 = tf.Variable(tf.constant(0.0, tf.float32, [num_emotions]))\n",
    "\n",
    "    y = tf.nn.softmax(tf.matmul(h_fc2_drop, w_fc3) + b_fc3)\n",
    "    \n",
    "    # Prediction function\n",
    "    prediction = tf.argmax(y, 1)\n",
    "    probabilities = y\n",
    "    \n",
    "    # Start TensorFlow session and assign learned weights and biases to model\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    w1_op = w1.assign(w1_val)\n",
    "    b1_op = b1.assign(b1_val)\n",
    "    w2_op = w2.assign(w2_val)\n",
    "    b2_op = b2.assign(b2_val)\n",
    "    w_fc1_op = w_fc1.assign(w_fc1_val)\n",
    "    b_fc1_op = b_fc1.assign(b_fc1_val)\n",
    "    w_fc2_op = w_fc2.assign(w_fc2_val)\n",
    "    b_fc2_op = b_fc2.assign(b_fc2_val)\n",
    "    w_fc3_op = w_fc3.assign(w_fc3_val)\n",
    "    b_fc3_op = b_fc3.assign(b_fc3_val)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run([w1_op, b1_op, w2_op, b2_op, w_fc1_op, b_fc1_op, w_fc2_op, b_fc2_op, w_fc3_op, b_fc3_op])\n",
    "    \n",
    "    # Run CNN model on test image\n",
    "    predicted_label = np.zeros(1)\n",
    "    predicted_prob = np.zeros(num_emotions)\n",
    "\n",
    "    image = np.array(img_pixels).reshape(1, img_pixels.shape[0])\n",
    "    \n",
    "    # Find predicted emotion label and probability distribution across emotion labels\n",
    "    predicted_label = prediction.eval(feed_dict={x: image, keep_prob: 1.0})\n",
    "    probabilities = probabilities.eval(feed_dict={x: image, keep_prob: 1.0})\n",
    "    \n",
    "    return predicted_label, probabilities\n",
    "\n",
    "file_path = \"/Users/Alex/Desktop/obama_tall.jpeg\"\n",
    "img = Image.open(file_path)\n",
    "\n",
    "# Resize image based on given base width\n",
    "img = resize_image(img)\n",
    "\n",
    "# Crop center of image\n",
    "img = crop_center(img)\n",
    "\n",
    "# Find image height and width\n",
    "img_width, img_height = image_size(img)\n",
    "\n",
    "# Convert image to flattened array of grayscale pixel values\n",
    "img_pixels = image_to_grayscale_pixel_values(img)\n",
    "\n",
    "# Standardize image pixels based on mean and standard deviations of pixels of training images\n",
    "img_pixels = standardize_pixels(img_pixels, pixel_mean, pixel_std)\n",
    "\n",
    "show_image = img_pixels.reshape(48,48)\n",
    "plt.imshow(show_image, cmap='gray')\n",
    "\n",
    "predicted_label, probabilities = CNN(img_pixels)\n",
    "print \"Predicted emotion: %s \\n\" % emotion_names[predicted_label][0]\n",
    "\n",
    "for i in range(num_emotions):\n",
    "    print \"%s: %.2f%%\" % (emotion_names[i], probabilities[0][i] * 100.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
